#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Oct  6 16:50:47 2022

@author: weng
Objective: prediction of ML prediction for AIMM score
"""

#% install packages
pip install missingno

#% import the packages
import numpy as np
import pandas as pd
import missingno as msno

#% import data: merged portfolio + aimm + gap + wdi

df = pd.read_csv("Data/master.csv")
df = pd.DataFrame()

df.head(5)
df.shape

#& should impute the missing data. (region average or time average)

#% describe the data (missing value and distribution)
nomi = df.isna().sum() < df.shape[0]/2 #% clean missing data: drop variables with 50% missing.
nomi.value_counts() #only 458 available

msno.bar(df) #plot the missing bar for viz

df = df.loc[:,nomi] #deleted the missing data

#% feature selection]
column = df.columns.values #identify the y variables from AIMM data. 


##numerical or categorical?
dfn = df.select_dtypes(include='number') 
dfn.shape[1] #430 numerical

##select numerical features first.(ANOVA correlation coefficient): reference: https://scikit-learn.org/stable/auto_examples/feature_selection/plot_feature_selection.html#sphx-glr-auto-examples-feature-selection-plot-feature-selection-py
from sklearn.model_selection import train_test_split

# The iris dataset
X, y = load_iris(return_X_y=True)

# Split dataset to select feature and evaluate the classifier
X = dfn.loc[:,~dfn.columns.str.contains('ex_ante', case=False)].to_numpy()
y = df['ex_ante_aimm_rating'].to_numpy()

df['ex_ante_aimm_rating'].value_counts() #there'son class only 1 value:79 drop it. 

X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=0) #convert categorical

from sklearn.feature_selection import SelectKBest, f_classif

selector = SelectKBest(f_classif, k=4)
selector.fit(X_train, y_train)
scores = -np.log10(selector.pvalues_) #delete missing
scores /= scores.max()

import matplotlib.pyplot as plt

X_indices = np.arange(X.shape[-1])
plt.figure(1)
plt.clf()
plt.bar(X_indices - 0.05, scores, width=0.2)
plt.title("Feature univariate score")
plt.xlabel("Feature number")
plt.ylabel(r"Univariate score ($-Log(p_{value})$)")
plt.show()
